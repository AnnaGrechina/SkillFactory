{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 определяем две функции:\n",
    "1. car_write_to_file(url, filename, car_id) - парсит одно объявление и добавляет данные в выходной файл\n",
    "2.  urls_write_to_file(FILENAME_URL) - создает перечень ссылок на объявления для конкретной марки,\n",
    "указанной в глобальной переменной brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Функция по парсингу одного объявления по известному URL в файл\n",
    "\n",
    "def car_write_to_file(url, filename, car_id):\n",
    "    global feature_name_kaggle, keys_list_1, keys_list_2, keys_2_site, feature_name_dict2\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    the_car_features = dict.fromkeys(feature_name_kaggle, '')\n",
    "    the_car_features['id'] = car_id\n",
    "    the_car_features['URL'] = url\n",
    "    try:\n",
    "        the_car_features['name'] = soup.select(\".CardBreadcrumbs__itemText\")[5].text[:-1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # набор признаков №1 (список 1)\n",
    "    for key in keys_list_1:\n",
    "        try:\n",
    "            the_car_features[key] = soup.find(itemprop = key).attrs['content']\n",
    "        except:\n",
    "            print(' НЕ НАЙДЕНО --- ', key, url)\n",
    "\n",
    "    \n",
    "    \n",
    "    # набор признаков №2 (список 2)\n",
    "    for li in soup.select(\".CardInfo > li\"):\n",
    "        feature_name_span, value_span = li.find_all('span')\n",
    "        try:\n",
    "            key = feature_name_dict2[feature_name_span.text]  # как на каггле называется этот параметр\n",
    "            the_car_features[key] = value_span.text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        txt = the_car_features['description']\n",
    "        the_car_features['description'] = '\"' + re.sub(r'[^A-zА-я0-9 ]', ' ', txt) + '\"'\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # в километраже оставить только цифры\n",
    "    try:\n",
    "        the_car_features['mileage'] = re.sub(r'[^0-9]', '', the_car_features['mileage'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Цена - оставить только цифры\n",
    "    try:\n",
    "        the_car_features['Price_RU'] = re.sub(r'[^0-9]', '', soup.select(\".OfferPriceCaption__price\")[0].text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Комплектация\n",
    "    try:\n",
    "        complectation_search = soup.find(\"script\", id=\"initial-state\").text\n",
    "        start = complectation_search.find('\"equipmentGroups\":')\n",
    "        end = complectation_search.rfind('},\"lk_summary\"')\n",
    "        if start != -1:\n",
    "            the_car_features['Комплектация'] = \"['\" + complectation_search[start + len('\"equipmentGroups\":'):end] + \"']\"\n",
    "        else:\n",
    "            the_car_features['Комплектация'] = []\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "  \n",
    "\n",
    "    text_list = []\n",
    "    for key in feature_name_kaggle:\n",
    "        text_list.append(the_car_features[key])\n",
    "    text = \"|\".join(map(str, text_list))\n",
    "    with open (FILENAME, 'a') as file:\n",
    "        file.write(text + '\\n')\n",
    "\n",
    "\n",
    "# Функция по созданию списка ссылок на объявления\n",
    "# проходит по регионам из списка cities_list, для каждого региона выбираются поочередно года с 1970 по 2019 \n",
    "# (цикл for year in range(1970, 2020))\n",
    "# и берет ссылки на конкретные объявления по этим страницам\n",
    "\n",
    "def urls_write_to_file(FILENAME_URL):\n",
    "    global brand\n",
    "    PARAMS_start = {'page' : '1',\n",
    "              'output_type' : 'list',\n",
    "              'sort' : 'fresh_relevance_1-desc'}\n",
    "\n",
    "    offer_count = 0\n",
    "    cities_list = ['adygeya', 'altay', 'bashkortostan', 'buryatiya', 'dagestan', 'ingushetiya', 'kabardino-balkariya', 'kalmykiya', 'karachaevo-cherkesiya',\n",
    "        'kareliya', 'komi', 'krym', 'mariy_el', 'mordoviya', 'saha_yakutiya', 'severnaya_osetiya', 'tatarstan', 'tyva',\n",
    "        'udmurtiya', 'hakasiya', 'chechenskaya_respublika', 'chuvashskiya', 'altayskiy_kray', 'zabaykalskiy_kray', 'kamchatskiy_kray',\n",
    "        'krasnodarskiy_kray', 'krasnoyarskiy_kray', 'permskiy_kray', 'primorskiy_kray', 'stavropolskiy_kray', 'habarovskiy_kray',\n",
    "        'amurskaya_oblast', 'arhangelskaya_oblast', 'astrahanskaya_oblast', 'belgorodskaya_oblast', 'bryanskaya_oblast', 'vladimirskaya_oblast',\n",
    "        'volgogradskaya_oblast', 'vologodskaya_oblast', 'voronezhskaya_oblast', 'ivanovskaya_oblast', 'irkutskaya_oblast',\n",
    "        'kaliningradskaya_oblast', 'kaluzhskaya_oblast', 'kemerovo', 'kirovskaya_oblast', 'kostromskaya_oblast', 'kurganskaya_oblast', \n",
    "        'kurskaya_oblast', 'leningradskaya_oblast', 'lipetskaya_oblast', 'magadanskaya_oblast', 'moskovskaya_oblast', 'murmanskaya_oblast',\n",
    "        'nizhegorodskaya_oblast', 'novgorodskaya_oblast', 'novosibirskaya_oblast', 'omskaya_oblast', 'orenburgskaya_oblast', 'orlovskaya_oblast',\n",
    "        'penzenskaya_oblast', 'pskovskaya_oblast', 'rostovskaya_oblast', 'ryazanskaya_oblast', 'samarskaya_oblast', 'saratovskaya_oblast',\n",
    "        'sahalinskaya_oblast', 'sverdlovskaya_oblast', 'smolenskaya_oblast', 'tambovskaya_oblast', 'tverskaya_oblast', 'tomskaya_oblast',\n",
    "        'tulskaya_oblast', 'tyumenskaya_oblast', 'ulyanovskaya_oblast', 'chelyabinskaya_oblast', 'yaroslavskaya_oblast',\n",
    "        'moskva', 'sankt-peterburg', 'sevastopol', 'nenetskiy_ao', 'hanty-mansiyskiy_ao', 'chukotskiy_ao', 'yamalo-nenetskiy_ao',\n",
    "        'vladimir', 'ryazan', 'nizhniy_novgorod', 'murom', 'tver', 'dubna', 'kolomna', 'tula', 'kaluga', 'vyazma', 'rzhev', 'smolensk',\n",
    "        'roslavl', 'bryansk', 'pskov', 'elets', 'lipetsk', 'tambov', 'penza', 'saransk', 'ulyanovsk', 'tolyatti', 'samara', 'kazan',\n",
    "        'yoshkar-ola', 'cheboksary', 'kostroma', 'yaroslavl', 'ivanovo', 'cherepovets', 'vologda', 'kirovskaya_oblast_kirov', 'perm',\n",
    "        'ekaterinburg', 'chelyabinsk', 'tyumen', 'ufa', 'krasnoyarsk', 'irkutsk']    # 'kaliningradskaya_oblast', \n",
    "    for city in cities_list:\n",
    "        print('city: ', city)\n",
    "        for year in range(1970, 2020):\n",
    "            URL = 'https://auto.ru/' + city + '/cars/' + str(brand) + '/' + str(year) + '-year/used/'\n",
    "            response = requests.get(URL, params = PARAMS_start)\n",
    "#            print (response.url)\n",
    "            soup = BeautifulSoup(response.content)\n",
    "            try:\n",
    "                offer_number = int(re.sub(r'[^0-9]', '', soup.select(\".ButtonWithLoader__content\")[0].text))\n",
    "            except:\n",
    "                offer_number = 0\n",
    "            offer_count += offer_number\n",
    "            page_number = offer_number // 37 + 1\n",
    "#            print('Total offers: {}, pages: {}'.format(offer_number, page_number))\n",
    "            for page in range (page_number):\n",
    "                PARAMS = {'page' : page,\n",
    "                          'output_type' : 'list',\n",
    "                          'sort' : 'fresh_relevance_1-desc'}\n",
    "                response = requests.get(URL, params = PARAMS)\n",
    "                soup = BeautifulSoup(response.content)\n",
    "                for elem in soup.find_all(\"meta\", itemprop = \"url\"):\n",
    "                    url_cur = elem.attrs['content']\n",
    "                    with open (FILENAME_URL, 'a') as file:\n",
    "                        file.write(city + ',' + url_cur + '\\n')\n",
    "    print('TOTAL OFFERS: ', offer_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Задаем глобальные переменные - список интересующих признаков и наименование бренда автомобиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем наименование полей для нашего датасета\n",
    "\n",
    "feature_name_kaggle = ['bodyType', 'brand', 'color', 'fuelType', 'modelDate', 'name',\n",
    "                       'numberOfDoors', 'productionDate', 'vehicleConfiguration',\n",
    "                       'vehicleTransmission', 'engineDisplacement', 'enginePower',\n",
    "                       'description', 'mileage', 'Комплектация', 'Привод', 'Руль', 'Состояние',\n",
    "                       'Владельцы', 'ПТС', 'Таможня', 'Владение', 'id',\n",
    "                      'price', 'URL']\n",
    "# определяем для удобства две группы полей, которые похожим образом парсятся на странце\n",
    "keys_list_1 =  ['bodyType', 'brand', 'color', 'fuelType', 'modelDate',\n",
    "              'numberOfDoors', 'productionDate', 'vehicleConfiguration',\n",
    "             'vehicleTransmission', 'engineDisplacement', 'enginePower',\n",
    "             'description', 'price']\n",
    "keys_list_2 = ['mileage', 'Привод', 'Руль', 'Состояние',\n",
    "            'Владельцы', 'ПТС', 'Таможня', 'Владение']\n",
    "keys_2_site = ['Пробег', 'Привод', 'Руль', 'Состояние', \n",
    "               'Владельцы', 'ПТС', 'Таможня', 'Владение']\n",
    "\n",
    "feature_name_dict2 = dict(zip(keys_2_site, keys_list_2))\n",
    "\n",
    "# задаем наименование бренда, который ищем в глобальной переменной brand\n",
    "\n",
    "brand = 'toyota'\n",
    "\n",
    "FILENAME = brand + '_train_data.csv'\n",
    "FILENAME_URL = brand + '_urls_list.csv'\n",
    "FILENAME_URL_unique = brand + '_URLs_unique.csv'\n",
    "car_id_continue = 0\n",
    "FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Основной скрипт:\n",
    "сначала считываем в файл все ссылки на интересующие объявления,\n",
    "\n",
    "потом идем по этому файлу и заполняем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (FILENAME_URL, 'w') as file:\n",
    "    file.write('city,offer_URL' + '\\n')\n",
    "\n",
    "# считываем в файл все ссылки на интересующие объявления,\n",
    "\n",
    "urls_write_to_file(FILENAME_URL)\n",
    "\n",
    "# прочитаем получившийся файл и удалим дубликаты:\n",
    "\n",
    "df_urls = pd.read_csv(FILENAME_URL)\n",
    "print('С дубликатами: ', df_urls.shape)\n",
    "df_urls = df_urls.drop_duplicates(subset = 'offer_URL')\n",
    "print('Осталось: ', df_urls.shape)\n",
    "\n",
    "df_urls.to_csv(FILENAME_URL_unique)\n",
    "\n",
    "# идем по этому файлу и заполняем датасет\n",
    "with open (FILENAME, 'w') as file:\n",
    "    file.write(\"|\".join(map(str, feature_name_kaggle)) + '\\n')\n",
    "\n",
    "# Если следующий цикл отработает с ошибкой и запишется только часть объявления, \n",
    "#то можно считать заново список уникальных URL и дополнять файл, \n",
    "# начиная с конкретного car_id_continue: df_urls = pd.read_csv(FILENAME_URL_unique)\n",
    "\n",
    "car_id = car_id_continue\n",
    "total_urls = df_urls.shape[0]\n",
    "while car_id < total_urls:\n",
    "    try:    \n",
    "        for url_cur in df_urls.iloc[car_id_continue:]['offer_URL']:\n",
    "            car_write_to_file(url_cur, FILENAME, car_id)\n",
    "            car_id += 1\n",
    "            time.sleep(1)\n",
    "            if car_id % 100 == 0: print('car_id: ', car_id)\n",
    "    except:\n",
    "        # если что-то пойдет не так, например разоврется соединение, то ждем 5 мин и пробуем продолжать\n",
    "        time.sleep(300)\n",
    "        my_data = pd.read_csv(FILENAME, encoding = 'ANSI', sep ='|')\n",
    "        car_id_continue = my_data.shape[0]\n",
    "        print('======  except block ==============')\n",
    "\n",
    "print('----end---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Если цикл по парсингу списка уникальных адресов (объявлений) завершился аварийно,\n",
    "то можно прочитать тот датасет, что смогли записать и продолжать с последнего объявления- вернутья в п.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(FILENAME, encoding = 'ANSI', sep ='|')\n",
    "car_id_continue = my_data.shape[0]\n",
    "car_id_continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
